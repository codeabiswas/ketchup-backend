# syntax=docker/dockerfile:1.4
FROM vllm/vllm-openai:v0.11.0

ENV HF_HOME=/model-cache
ENV TRANSFORMERS_CACHE=/model-cache
ENV HF_HUB_DISABLE_TELEMETRY=1

RUN --mount=type=secret,id=HF_TOKEN python3 - <<'PY'
import inspect
from huggingface_hub import snapshot_download

token = open("/run/secrets/HF_TOKEN", "r").read().strip()

kwargs = dict(
    repo_id="Qwen/Qwen3-4B-Instruct-2507",
    local_dir="/model-cache/Qwen3-4B-Instruct-2507",
    token=token,
)

sig = inspect.signature(snapshot_download)
if "local_dir_use_symlinks" in sig.parameters:
    kwargs["local_dir_use_symlinks"] = False

snapshot_download(**kwargs)
PY

ENV HF_HUB_OFFLINE=1

WORKDIR /srv
COPY app/ /srv/app/
RUN pip install --no-cache-dir -r /srv/app/requirements.txt
RUN chmod +x /srv/app/entrypoint.sh
ENTRYPOINT ["/srv/app/entrypoint.sh"]